{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f2b8be6-b0a4-4012-984c-f122993085b6",
   "metadata": {},
   "source": [
    "This notebook shows how you can test your model for inference. You can use this notebook as a starting point for your inference activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c20adf63-d985-4893-a311-c5573c30b72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from mlflow import MlflowClient\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c5f5da0-57f9-4607-9661-5e0be0a92dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "{% if cookiecutter.use_minio == 'yes' %}\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = os.getenv(\"J_MLFLOW_S3_ENDPOINT_URL\")\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be57160f-89b6-4d41-9fd8-fcba0175815a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifact URI: /mlflow_artifacts/0/998e099fd5f2480eb5466396e6419f25/artifacts/iris_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/31 11:34:37 WARNING mlflow.utils.requirements_utils: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - mlflow (current: 2.20.0, required: mlflow==2.20.1)\n",
      " - numpy (current: 2.2.2, required: numpy==1.26.4)\n",
      " - pandas (current: 2.2.2, required: pandas==2.1.4)\n",
      " - psutil (current: 6.1.1, required: psutil==6.1.0)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n",
      "Prediction: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yogesh/miniforge3/envs/mlops/lib/python3.12/site-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def get_model_for_inference(model_name: str, version: str) -> (\n",
    "        mlflow.pyfunc.PyFuncModel):\n",
    "    \"\"\"\n",
    "    This function gets the model from the MLFlow registry based on the model_name\n",
    "    and version provided.\n",
    "    \"\"\"\n",
    "    model_version = client.get_model_version(model_name, version)\n",
    "    artifact_uri = model_version.source\n",
    "    {% if cookiecutter.use_minio == 'yes' %}\n",
    "    print(f\"Artifact URI: {artifact_uri}\")\n",
    "    loaded_model = mlflow.pyfunc.load_model(artifact_uri)\n",
    "    {% else %}\n",
    "    final_artifact_path = artifact_uri.replace(\"/mlflow_artifacts\", \"./mlflow_artifacts\", 1)\n",
    "    print(f\"Artifact URI: {final_artifact_path}\")\n",
    "    loaded_model = mlflow.pyfunc.load_model(\"../../\" + final_artifact_path)\n",
    "    {% endif %}\n",
    "    print(\"model loaded\")\n",
    "    return loaded_model\n",
    "\n",
    "# This is important as we need to tell this local MLFlow that there is already a MLFlow instance running \n",
    "# in docker exposed at this URI: J_MLFLOW_SERVER_URI\n",
    "mlflow.set_tracking_uri(os.getenv('J_MLFLOW_SERVER_URI'))\n",
    "client = MlflowClient(mlflow.get_tracking_uri())\n",
    "\n",
    "# Provide your model name and the version you would like to test\n",
    "model_name = \"tiny_iris_classifier\"\n",
    "version = \"1\"\n",
    "\n",
    "loaded_model = get_model_for_inference(model_name, version)\n",
    "\n",
    "# Prepare your data for inference\n",
    "input_data = sample_data = pd.DataFrame([[5.1, 3.5, 1.4, 0.2]],\n",
    "      columns=[\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\", \"petal width (cm)\"])\n",
    "\n",
    "# Perform inference\n",
    "prediction = loaded_model.predict(pd.DataFrame(input_data))\n",
    "print(\"Prediction:\", prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
